---
phase: 06-automation-safety
plan: 04
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - backend/src/jobs/processors/stagingProcessor.ts
  - backend/src/jobs/queues.ts
  - backend/src/services/eventCollector.ts
  - backend/src/config/socket.ts
autonomous: true
requirements: [SAFE-01, SAFE-02]

must_haves:
  truths:
    - "Staging processor executes expired staged items every 30 minutes"
    - "Rule engine evaluates incoming emails during webhook event processing"
    - "Socket.IO emits staging notifications for real-time badge updates"
  artifacts:
    - path: "backend/src/jobs/processors/stagingProcessor.ts"
      provides: "BullMQ processor for expired staged items"
      exports: ["processStagingItems"]
    - path: "backend/src/services/eventCollector.ts"
      provides: "Rule engine integration in handleCreated"
      contains: "evaluateRulesForMessage"
  key_links:
    - from: "backend/src/services/eventCollector.ts"
      to: "backend/src/services/ruleEngine.ts"
      via: "evaluateRulesForMessage call in handleCreated"
      pattern: "evaluateRulesForMessage"
    - from: "backend/src/jobs/processors/stagingProcessor.ts"
      to: "backend/src/services/actionExecutor.ts"
      via: "executeActions for expired items"
      pattern: "graphFetch.*move|PATCH"
    - from: "backend/src/jobs/queues.ts"
      to: "backend/src/jobs/processors/stagingProcessor.ts"
      via: "processorMap replacement"
      pattern: "processStagingItems"
---

<objective>
Wire the rule engine into the webhook event pipeline, implement the staging processor BullMQ job, and add Socket.IO staging notifications -- completing the end-to-end automation flow.

Purpose: This plan connects all the services from Plans 01-02 into working pipelines. When an email arrives, the rule engine evaluates it. When staged items expire, the staging processor executes them. Socket.IO keeps the frontend in sync.

Output: Staging processor replacing the placeholder, eventCollector integration with rule engine, Socket.IO staging events.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-automation-safety/06-RESEARCH.md
@.planning/phases/06-automation-safety/06-01-SUMMARY.md

@backend/src/services/eventCollector.ts
@backend/src/jobs/queues.ts
@backend/src/jobs/processors/webhookEvents.ts
@backend/src/config/socket.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Staging processor BullMQ job</name>
  <files>
    backend/src/jobs/processors/stagingProcessor.ts
    backend/src/jobs/queues.ts
  </files>
  <action>
    **1. Create `backend/src/jobs/processors/stagingProcessor.ts`:**
    - Import `Job` from `bullmq`
    - Import `StagedEmail` from `../../models/StagedEmail.js`
    - Import `Mailbox` from `../../models/Mailbox.js`
    - Import `AuditLog` from `../../models/AuditLog.js`
    - Import `getAccessTokenForMailbox` from `../../auth/tokenManager.js`
    - Import `graphFetch, GraphApiError` from `../../services/graphClient.js`
    - Import logger

    **Export `processStagingItems(job: Job): Promise<void>`:**
    - Find expired staged items: `StagedEmail.find({ status: 'staged', expiresAt: { $lte: new Date() } }).limit(100)`
    - If none found, log debug and return early
    - Process items with concurrency limit of 5 (use a simple batching approach, not Promise.all on all items):
      - Split items into chunks of 5
      - For each chunk, use `Promise.allSettled` to execute in parallel
    - For each item:
      - Get access token: `getAccessTokenForMailbox(item.mailboxId.toString())`
      - Get mailbox email: `Mailbox.findById(item.mailboxId).select('email')`
      - Execute the staged action based on `item.actions`:
        - For 'delete' action: Move email from staging folder to Deleted Items via `POST /users/{email}/messages/{messageId}/move` with `{ destinationId: 'deleteditems' }` (using well-known folder name)
        - For other actions: Execute the corresponding Graph API call (move, markRead, etc.)
      - Handle 404 gracefully: message may have been manually deleted or rescued. Log warning, set status to 'expired' instead of 'executed'
      - Handle 429 (rate limit): Check `Retry-After` header, log warning, skip this item (will be picked up in next run)
      - On success: Update `item.status = 'executed'`, `item.executedAt = new Date()`, save
      - Create AuditLog entry: action 'email_executed', targetType 'email', targetId messageId, details `{ ruleId: item.ruleId, actions: item.actions, originalFolder: item.originalFolder }`, undoable true, mailboxId
      - On failure (non-404, non-429): Log error, leave status as 'staged' for retry on next run
    - Log summary: items processed, successes, failures

    **2. Update `backend/src/jobs/queues.ts`:**
    - Add import: `import { processStagingItems } from './processors/stagingProcessor.js';`
    - Replace the placeholder in processorMap:
      ```
      'staging-processor': processStagingItems,
      ```
    - Remove the now-unused `createProcessor('staging-processor')` call (only if staging-processor was the last remaining placeholder user)
  </action>
  <verify>
    Run `npx tsc --noEmit` from backend directory.
    Verify stagingProcessor.ts exports processStagingItems.
    Verify queues.ts processorMap has `processStagingItems` (not placeholder).
    Grep for 'permanentDelete' in stagingProcessor.ts -- zero matches.
    Grep for 'createProcessor' in queues.ts -- should only exist if other queues still use it (in this case, none should).
  </verify>
  <done>
    Staging processor replaces placeholder, processes expired items every 30 minutes.
    Handles rate limiting (429) and missing messages (404) gracefully.
    Creates AuditLog entries for each executed item with undoable flag.
    Uses soft-delete only (move to deleteditems, never permanentDelete).
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate rule engine into webhook event pipeline</name>
  <files>
    backend/src/services/eventCollector.ts
  </files>
  <action>
    **Modify `backend/src/services/eventCollector.ts` to add rule evaluation after event processing:**

    - Add imports at top:
      - `import { evaluateRulesForMessage } from './ruleEngine.js';`
      - `import { executeActions } from './actionExecutor.js';`
      - `import type { Types } from 'mongoose';`

    - Modify the `handleCreated` function to evaluate rules after saving the email event:
      - After the existing `await saveEmailEvent(eventData)` call, add rule evaluation:
      ```typescript
      // Evaluate automation rules for the new message
      try {
        const result = await evaluateRulesForMessage(
          userId as Types.ObjectId,
          mailboxId as Types.ObjectId,
          graphMessage,
          accessToken,
        );

        if (result.matched && result.ruleId && result.actions) {
          logger.info('Rule matched incoming email', {
            ruleId: result.ruleId,
            messageId,
            actionsCount: result.actions.length,
          });

          await executeActions({
            mailboxEmail,
            messageId,
            actions: result.actions,
            ruleId: result.ruleId,
            userId: userId as Types.ObjectId,
            mailboxId: mailboxId as Types.ObjectId,
            originalFolder: graphMessage.parentFolderId,
            accessToken,
          });
        }
      } catch (err) {
        // Rule evaluation failures must not block event collection
        logger.error('Rule evaluation failed for incoming email', {
          messageId,
          error: err instanceof Error ? err.message : String(err),
        });
      }
      ```

    - This integration is CRITICAL: the rule engine runs inline with webhook event processing (not in a separate queue). This keeps latency low. If rule evaluation fails, the email event is still saved (error is caught and logged).

    - The try-catch around rule evaluation ensures that rule engine errors never prevent email event recording. The observation pipeline must remain reliable even if automation fails.

    **Important notes:**
    - Do NOT add rule evaluation to handleUpdated or handleDeleted -- rules only fire on new emails (created events)
    - The rule engine's own kill switch and whitelist checks happen inside evaluateRulesForMessage
    - Socket.IO staging notifications are already handled by stagingManager.createStagedEmail (from Plan 01)
  </action>
  <verify>
    Run `npx tsc --noEmit` from backend directory.
    Grep for 'evaluateRulesForMessage' in eventCollector.ts -- should find the import and call.
    Grep for 'executeActions' in eventCollector.ts -- should find the import and call.
    Verify the rule evaluation is wrapped in try-catch (error isolation).
    Verify rule evaluation only happens in handleCreated (not handleUpdated or handleDeleted).
  </verify>
  <done>
    Rule engine evaluates incoming emails during webhook event processing.
    Rule evaluation failures are isolated -- email events are still recorded.
    Matched rules trigger action execution (with staging for destructive actions).
    The end-to-end flow works: email arrives -> event saved -> rules evaluated -> actions executed (or staged).
  </done>
</task>

</tasks>

<verification>
- `npx tsc --noEmit` passes from backend/
- Staging processor is production-ready (not placeholder)
- Rule engine is wired into handleCreated in eventCollector
- All 6 BullMQ queues now have production processors
- No permanentDelete anywhere in the codebase
</verification>

<success_criteria>
- Staging processor executes expired items with rate limit handling
- Rule engine fires on incoming emails and triggers appropriate actions
- Error isolation ensures observation pipeline works even if automation fails
- End-to-end flow: email arrival -> rule match -> action execution (or staging)
</success_criteria>

<output>
After completion, create `.planning/phases/06-automation-safety/06-04-SUMMARY.md`
</output>
