---
phase: 01-infrastructure-foundation
plan: 02
type: execute
wave: 2
depends_on:
  - 01-01
files_modified:
  - backend/package.json
  - backend/src/config/database.ts
  - backend/src/config/redis.ts
  - backend/src/models/index.ts
  - backend/src/models/User.ts
  - backend/src/models/EmailEvent.ts
  - backend/src/models/Pattern.ts
  - backend/src/models/Rule.ts
  - backend/src/models/StagedEmail.ts
  - backend/src/models/AuditLog.ts
  - backend/src/models/Notification.ts
  - backend/src/models/WebhookSubscription.ts
  - backend/src/models/Mailbox.ts
  - backend/src/jobs/queues.ts
  - backend/src/jobs/schedulers.ts
  - backend/src/server.ts
autonomous: true
requirements:
  - INFR-03
must_haves:
  truths:
    - "Backend connects to MongoDB on startup with retry logic and logs successful connection"
    - "Backend connects to Redis on startup and confirms PONG response"
    - "All 9 Mongoose models are registered and compound indexes are created"
    - "Five BullMQ queues are initialized with removeOnComplete/removeOnFail age limits"
    - "Five job schedulers are registered via upsertJobScheduler with correct intervals"
    - "A test job can be enqueued and processed by a worker"
    - "MongoDB connection recovers after a container restart (retry logic works)"
  artifacts:
    - path: "backend/src/config/database.ts"
      provides: "Mongoose connection with exponential backoff retry"
      contains: "connectDatabase"
    - path: "backend/src/config/redis.ts"
      provides: "ioredis connections for queues and workers"
      contains: "maxRetriesPerRequest: null"
    - path: "backend/src/models/User.ts"
      provides: "User model with email unique index"
      contains: "model.*User"
    - path: "backend/src/models/EmailEvent.ts"
      provides: "EmailEvent model with compound indexes and TTL"
      contains: "expireAfterSeconds"
    - path: "backend/src/jobs/queues.ts"
      provides: "Five BullMQ queue + worker definitions"
      contains: "webhook-renewal"
    - path: "backend/src/jobs/schedulers.ts"
      provides: "Five job schedulers via upsertJobScheduler"
      contains: "upsertJobScheduler"
  key_links:
    - from: "backend/src/server.ts"
      to: "backend/src/config/database.ts"
      via: "await connectDatabase() before listen"
      pattern: "connectDatabase"
    - from: "backend/src/server.ts"
      to: "backend/src/jobs/schedulers.ts"
      via: "await initializeSchedulers() after database"
      pattern: "initializeSchedulers"
    - from: "backend/src/jobs/queues.ts"
      to: "backend/src/config/redis.ts"
      via: "shared Redis connections"
      pattern: "import.*redis"
---

<objective>
Connect the backend to MongoDB and Redis, define all 9 Mongoose models with compound indexes, and initialize BullMQ queues with job schedulers for all 5 recurring background jobs.

Purpose: Establish the persistence and job processing layers that authentication (Phase 2), observation (Phase 3), and pattern detection (Phase 5) all depend on. Models defined now mean Phase 2 can immediately write user and token data.

Output: Backend connects to MongoDB and Redis on startup, all models are registered with indexes, and BullMQ queues are active with schedulers registered.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-infrastructure-foundation/01-RESEARCH.md
@.planning/phases/01-infrastructure-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: MongoDB connection, Redis connection, and all Mongoose models</name>
  <files>
    backend/package.json
    backend/src/config/database.ts
    backend/src/config/redis.ts
    backend/src/models/index.ts
    backend/src/models/User.ts
    backend/src/models/EmailEvent.ts
    backend/src/models/Pattern.ts
    backend/src/models/Rule.ts
    backend/src/models/StagedEmail.ts
    backend/src/models/AuditLog.ts
    backend/src/models/Notification.ts
    backend/src/models/WebhookSubscription.ts
    backend/src/models/Mailbox.ts
  </files>
  <action>
    First, add new dependencies to backend/package.json:
    ```
    npm install mongoose@8 ioredis@5 bullmq@5
    ```

    **backend/src/config/database.ts**:
    - Export `connectDatabase(): Promise<void>` with exponential backoff retry (max 10 retries, base delay 1000ms, max delay 30000ms)
    - Use connection URI from config (MONGODB_URI, default mongodb://msedb-mongo:27017/msedb)
    - Connection options: maxPoolSize 50, minPoolSize 5, serverSelectionTimeoutMS 5000, socketTimeoutMS 45000, family 4
    - Log connection success via logger
    - Register event handlers: 'error' (log error), 'disconnected' (log warning about auto-reconnect)
    - On final retry failure, throw the error
    - See research Pattern 1 for implementation reference

    **backend/src/config/redis.ts**:
    - Export two ioredis connection factories: `createQueueConnection()` and `createWorkerConnection()`
    - Queue connection: host from REDIS_HOST (default msedb-redis), port from REDIS_PORT (default 6379), enableOfflineQueue false
    - Worker connection: same host/port but with `maxRetriesPerRequest: null` (REQUIRED for BullMQ workers)
    - Export a `getRedisClient()` function that returns a general-purpose ioredis client (for health checks, rate limiting later)
    - Log connection status via logger
    - Handle 'error' events on all connections (log, do not crash)

    **All 9 Mongoose Models** -- create each as a separate file with TypeScript interface, Schema, and exported model:

    1. **User.ts**: userId (auto), email (string, required, unique), microsoftId (string, unique, sparse), displayName, role (enum: 'admin'|'user', default 'user'), isActive (boolean, default true), preferences subdoc (automationPaused boolean default false, workingHoursStart/End numbers, aggressiveness enum 'conservative'|'moderate'|'aggressive' default 'moderate'), encryptedTokens subdoc (accessToken object with encrypted/iv/tag strings, refreshToken same, expiresAt Date), msalCache (String -- serialized MSAL cache for ICachePlugin), invitedBy (ObjectId ref User), lastLoginAt (Date). Indexes: { email: 1 } unique, { microsoftId: 1 } unique sparse.

    2. **Mailbox.ts**: userId (ObjectId ref User, required), email (string, required), displayName, tenantId, isConnected (boolean, default true), encryptedTokens (same structure as User), msalCache (String), lastSyncAt (Date), deltaLinks (Map of string -- folder-specific delta tokens stored in MongoDB not Redis for persistence), settings subdoc (automationPaused boolean, whitelistedSenders [string], whitelistedDomains [string]). Indexes: { userId: 1, email: 1 } unique compound, { userId: 1 }.

    3. **EmailEvent.ts**: userId (ObjectId ref User, required), mailboxId (ObjectId ref Mailbox, required), messageId (string, required), internetMessageId (string), eventType (enum: 'arrived'|'deleted'|'moved'|'read'|'flagged'|'categorized', required), timestamp (Date, default now), sender subdoc (name string, email string, domain string), subject (string), subjectNormalized (string), receivedAt (Date), timeToAction (number -- seconds), fromFolder (string), toFolder (string), importance (enum 'low'|'normal'|'high', default 'normal'), hasAttachments (boolean, default false), conversationId (string), categories ([string]), isRead (boolean, default false), metadata subdoc (hasListUnsubscribe boolean, isNewsletter boolean, isAutomated boolean, automatedByRule ObjectId ref Rule). Indexes: { userId: 1, 'sender.domain': 1, timestamp: -1 }, { userId: 1, eventType: 1, timestamp: -1 }, { userId: 1, mailboxId: 1, messageId: 1, eventType: 1 } unique (dedup), { timestamp: 1 } with expireAfterSeconds 90 * 24 * 60 * 60 (90-day TTL). Timestamps: true.

    4. **Pattern.ts**: userId (ObjectId ref User, required), mailboxId (ObjectId ref Mailbox, required), patternType (enum 'sender'|'folder-routing', required), status (enum 'detected'|'suggested'|'approved'|'rejected'|'expired', default 'detected'), confidence (number, required, min 0, max 100), sampleSize (number, required), exceptionCount (number, default 0), condition subdoc (senderEmail string, senderDomain string, fromFolder string, subjectPattern string), suggestedAction subdoc (actionType enum 'delete'|'move'|'archive'|'markRead'|'flag'|'categorize', toFolder string, category string), evidence ([{ messageId string, timestamp Date, action string }], max 10), rejectedAt (Date), rejectionCooldownUntil (Date), approvedAt (Date), lastAnalyzedAt (Date). Indexes: { userId: 1, mailboxId: 1, status: 1 }, { userId: 1, patternType: 1, 'condition.senderDomain': 1 }, { rejectionCooldownUntil: 1 } sparse. Timestamps: true.

    5. **Rule.ts**: userId (ObjectId ref User, required), mailboxId (ObjectId ref Mailbox, required), name (string, required), sourcePatternId (ObjectId ref Pattern), isEnabled (boolean, default true), priority (number, default 0), conditions subdoc (senderEmail string, senderDomain string, subjectContains string, fromFolder string), actions ([{ actionType enum 'move'|'delete'|'markRead'|'flag'|'categorize'|'archive', toFolder string, category string, order number }]), stats subdoc (totalExecutions number default 0, lastExecutedAt Date, emailsProcessed number default 0), graphRuleId (string -- Microsoft Graph rule ID if synced), scope (enum 'user'|'org', default 'user'), createdBy (ObjectId ref User). Indexes: { userId: 1, mailboxId: 1, isEnabled: 1, priority: 1 }, { graphRuleId: 1 } sparse. Timestamps: true.

    6. **StagedEmail.ts**: userId (ObjectId ref User, required), mailboxId (ObjectId ref Mailbox, required), ruleId (ObjectId ref Rule, required), messageId (string, required), originalFolder (string, required), stagedAt (Date, default now), expiresAt (Date, required -- 24h after stagedAt), status (enum 'staged'|'executed'|'rescued'|'expired', default 'staged'), actions ([{ actionType string, toFolder string }]), executedAt (Date), rescuedAt (Date). Indexes: { userId: 1, status: 1, expiresAt: 1 }, { expiresAt: 1 } with expireAfterSeconds 0 (TTL at exact date). Timestamps: true.

    7. **AuditLog.ts**: userId (ObjectId ref User, required), mailboxId (ObjectId ref Mailbox), action (enum 'rule_created'|'rule_updated'|'rule_deleted'|'rule_executed'|'email_staged'|'email_rescued'|'email_executed'|'pattern_approved'|'pattern_rejected'|'automation_paused'|'automation_resumed'|'undo_action'|'whitelist_updated', required), targetType (enum 'email'|'rule'|'pattern'|'settings', required), targetId (string), details (Schema.Types.Mixed -- flexible object for action-specific data), undoable (boolean, default false), undoneAt (Date), undoneBy (ObjectId ref User). Indexes: { userId: 1, action: 1, createdAt: -1 }, { userId: 1, mailboxId: 1, createdAt: -1 }, { targetType: 1, targetId: 1 }. Timestamps: true.

    8. **Notification.ts**: userId (ObjectId ref User, required), type (enum 'pattern_detected'|'rule_executed'|'staging_alert'|'system'|'inactivity_warning', required), title (string, required), message (string, required), isRead (boolean, default false), readAt (Date), relatedEntity subdoc (entityType enum 'pattern'|'rule'|'staged_email', entityId ObjectId), priority (enum 'low'|'normal'|'high', default 'normal'). Indexes: { userId: 1, isRead: 1, createdAt: -1 }, { createdAt: 1 } with expireAfterSeconds 30 * 24 * 60 * 60 (30-day TTL). Timestamps: true.

    9. **WebhookSubscription.ts**: userId (ObjectId ref User, required), mailboxId (ObjectId ref Mailbox, required), subscriptionId (string, required, unique -- Graph subscription ID), resource (string, required -- e.g., '/me/mailFolders/{id}/messages'), changeType (string, required -- 'created,updated,deleted'), expiresAt (Date, required), notificationUrl (string, required), lifecycleNotificationUrl (string), clientState (string, required -- HMAC secret for validation), status (enum 'active'|'expired'|'failed', default 'active'), lastNotificationAt (Date), errorCount (number, default 0). Indexes: { subscriptionId: 1 } unique, { userId: 1, mailboxId: 1 }, { expiresAt: 1, status: 1 }. Timestamps: true.

    **backend/src/models/index.ts** (barrel export):
    - Import and re-export all 9 models
    - Export all interfaces (IUser, IMailbox, IEmailEvent, IPattern, IRule, IStagedEmail, IAuditLog, INotification, IWebhookSubscription)
  </action>
  <verify>
    1. `cd backend && npx tsc --noEmit` compiles without errors
    2. After `docker compose up --build -d`, check MongoDB connection log: `docker compose logs msedb-backend | grep -i "mongodb connected"`
    3. Verify models are registered: `docker compose exec msedb-mongo mongosh msedb --eval "db.getCollectionNames()"` should show collections after first write
    4. Verify indexes: `docker compose exec msedb-mongo mongosh msedb --eval "db.emailevents.getIndexes()"` should show compound indexes
  </verify>
  <done>
    MongoDB connection with retry logic is established. Redis connections for queues and workers are configured. All 9 Mongoose models are registered with TypeScript interfaces, compound indexes, and TTL indexes. Backend compiles without TypeScript errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: BullMQ queues, workers, and job schedulers</name>
  <files>
    backend/src/jobs/queues.ts
    backend/src/jobs/schedulers.ts
    backend/src/server.ts
  </files>
  <action>
    **backend/src/jobs/queues.ts**:
    - Import Queue and Worker from 'bullmq'
    - Import Redis connection factories from config/redis.ts
    - Create a shared queue connection using createQueueConnection()
    - Define default job options: removeOnComplete { age: 3600, count: 200 }, removeOnFail { age: 86400, count: 1000 }
    - Create 5 queues with these names: 'webhook-renewal', 'delta-sync', 'pattern-analysis', 'staging-processor', 'token-refresh'
    - Export all queues as a named object
    - Create 5 workers, one per queue, each using createWorkerConnection() (separate connection per worker)
    - Each worker processor logs job start/completion for now (placeholder): `logger.info('Processing job', { queue: name, jobId: job.id })`
    - Add 'completed' and 'failed' event handlers on each worker for logging
    - Export a `closeAllWorkers()` function for graceful shutdown (calls worker.close() on each)
    - Export a `closeAllQueues()` function for graceful shutdown (calls queue.close() on each)

    **backend/src/jobs/schedulers.ts**:
    - Import queues from queues.ts
    - Export `initializeSchedulers(): Promise<void>` that calls upsertJobScheduler on each queue:
      1. webhook-renewal: scheduler ID 'webhook-renewal-schedule', pattern '0 */2 * * *' (every 2h), job name 'renew-webhooks'
      2. delta-sync: scheduler ID 'delta-sync-schedule', every 15 * 60 * 1000 (15 min), job name 'run-delta-sync'
      3. pattern-analysis: scheduler ID 'pattern-analysis-schedule', pattern '0 2 * * *' (daily 2 AM), job name 'analyze-patterns'
      4. staging-processor: scheduler ID 'staging-processor-schedule', every 30 * 60 * 1000 (30 min), job name 'process-staging'
      5. token-refresh: scheduler ID 'token-refresh-schedule', every 45 * 60 * 1000 (45 min), job name 'refresh-tokens'
    - Each scheduler should set opts: { attempts: 3, backoff: { type: 'exponential', delay: 5000 } }
    - Log each scheduler registration via logger
    - See research "BullMQ All Job Schedulers" for implementation reference

    **Update backend/src/server.ts**:
    - Import connectDatabase from config/database.ts
    - Import getRedisClient from config/redis.ts
    - Import initializeSchedulers from jobs/schedulers.ts
    - Import closeAllWorkers, closeAllQueues from jobs/queues.ts
    - Import all models via models/index.ts (triggers model registration)
    - In startup: await connectDatabase(), then verify Redis with getRedisClient().ping(), then await initializeSchedulers(), then start Express server
    - Store redis client on app: app.set('redis', redisClient) for health checks later
    - Add graceful shutdown handler (SIGTERM, SIGINT): close workers, close queues, disconnect mongoose, close redis, then process.exit(0)
    - The server should only start listening AFTER database and Redis are connected and schedulers are initialized

    Rebuild and verify:
    - `docker compose up --build -d`
    - Check logs for scheduler registration messages
    - Manually enqueue a test job and verify processing
  </action>
  <verify>
    1. `docker compose logs msedb-backend | grep -i "scheduler"` shows all 5 schedulers registered
    2. `docker compose logs msedb-backend | grep -i "redis"` shows successful Redis connection
    3. `docker compose exec msedb-redis redis-cli KEYS "bull:*"` shows BullMQ keys exist
    4. Test job processing: `docker compose exec msedb-redis redis-cli` then check queue keys are present
    5. Verify noeviction: `docker compose exec msedb-redis redis-cli CONFIG GET maxmemory-policy` returns "noeviction"
    6. `docker compose restart msedb-mongo && sleep 15 && docker compose logs msedb-backend --since 30s | grep -i "mongodb"` shows reconnection (retry logic works)
  </verify>
  <done>
    Five BullMQ queues (webhook-renewal, delta-sync, pattern-analysis, staging-processor, token-refresh) are initialized with removeOnComplete/removeOnFail age limits. Five job schedulers are registered via upsertJobScheduler with correct intervals. Workers are active and processing. Redis confirms noeviction policy. MongoDB reconnection retry logic works after container restart. Graceful shutdown closes all connections.
  </done>
</task>

</tasks>

<verification>
1. Backend connects to MongoDB with retry logic on startup
2. Backend connects to Redis and confirms PONG
3. All 9 Mongoose models compile and are registered (TypeScript compiles clean)
4. Five BullMQ queues exist in Redis with correct names
5. Five job schedulers are registered with correct intervals
6. Workers log job processing activity
7. MongoDB container restart triggers reconnection (retry logic)
8. Graceful shutdown closes all connections cleanly
</verification>

<success_criteria>
- MongoDB connection with exponential backoff retry is established on startup
- Redis connection with noeviction policy is confirmed
- All 9 Mongoose models (User, Mailbox, EmailEvent, Pattern, Rule, StagedEmail, AuditLog, Notification, WebhookSubscription) are registered with compound indexes
- 5 BullMQ queues are initialized with removeOnComplete/removeOnFail age limits
- 5 job schedulers are registered via upsertJobScheduler with correct cron/interval patterns
- Backend compiles without TypeScript errors
</success_criteria>

<output>
After completion, create `.planning/phases/01-infrastructure-foundation/01-02-SUMMARY.md`
</output>
